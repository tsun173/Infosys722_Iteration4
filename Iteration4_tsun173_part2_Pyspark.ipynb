{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2.4 Verify the data quality\n",
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-2.1.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "import pandas as pd\n",
    "\n",
    "data_2014_8attrs = pd.read_csv('./soilCSV_2014_8attrs_500000.csv', sep=',')\n",
    "spark_init = SparkSession.builder.appName('pandasToSparkDF').getOrCreate()\n",
    "\n",
    "df_2014_8attrs_spark = spark_init.createDataFrame(data_2014_8attrs)\n",
    "df_2014_8attrs_spark.filter(df_2014_8attrs_spark[\"Soil_Class\"].isNull() | \n",
    "                            df_2014_8attrs_spark[\"Coastal\"].isNull() |\n",
    "                            df_2014_8attrs_spark[\"Blue\"].isNull() |\n",
    "                            df_2014_8attrs_spark[\"Green\"].isNull() |\n",
    "                            df_2014_8attrs_spark[\"Red\"].isNull() |\n",
    "                            df_2014_8attrs_spark[\"NIR\"].isNull() |\n",
    "                            df_2014_8attrs_spark[\"SWIR1\"].isNull() |\n",
    "                            df_2014_8attrs_spark[\"SWIR2\"].isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2018_8attrs = pd.read_csv('./soilCSV_2018_8attrs_500000.csv', sep=',')\n",
    "\n",
    "df_2018_8attrs_spark = spark_init.createDataFrame(data_2018_8attrs)\n",
    "df_2018_8attrs_spark.filter(df_2018_8attrs_spark[\"Soil_Class\"].isNull() | \n",
    "                            df_2018_8attrs_spark[\"Coastal\"].isNull() |\n",
    "                            df_2018_8attrs_spark[\"Blue\"].isNull() |\n",
    "                            df_2018_8attrs_spark[\"Green\"].isNull() |\n",
    "                            df_2018_8attrs_spark[\"Red\"].isNull() |\n",
    "                            df_2018_8attrs_spark[\"NIR\"].isNull() |\n",
    "                            df_2018_8attrs_spark[\"SWIR1\"].isNull() |\n",
    "                            df_2018_8attrs_spark[\"SWIR2\"].isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-2.1.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "\n",
    "spark_init = SparkSession.builder.appName('pandasToSparkDF').getOrCreate()\n",
    "dataset_2014_moreAttr = pd.read_csv('./soilCSV_2014_500000.csv', sep=',')\n",
    "\n",
    "df_2014_spark = spark_init.createDataFrame(dataset_2014_moreAttr)\n",
    "df_2014_spark.filter(df_2014_spark[\"Soil_Class\"].isNull() | \n",
    "                            df_2014_spark[\"Coastal\"].isNull() |\n",
    "                            df_2014_spark[\"Blue\"].isNull() |\n",
    "                            df_2014_spark[\"Green\"].isNull() |\n",
    "                            df_2014_spark[\"Red\"].isNull() |\n",
    "                            df_2014_spark[\"NIR\"].isNull() |\n",
    "                            df_2014_spark[\"SWIR1\"].isNull() |\n",
    "                            df_2014_spark[\"SWIR2\"].isNull() |\n",
    "                            df_2014_spark[\"NDVI\"].isNull() |\n",
    "                            df_2014_spark[\"NDWI\"].isNull() |\n",
    "                            df_2014_spark[\"NDMI\"].isNull() |\n",
    "                            df_2014_spark[\"RVI\"].isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_2018_moreAttr = pd.read_csv('./soilCSV_2018_500000.csv', sep=',')\n",
    "\n",
    "df_2018_spark = spark_init.createDataFrame(dataset_2018_moreAttr)\n",
    "df_2018_spark.filter(df_2018_spark[\"Soil_Class\"].isNull() | \n",
    "                            df_2018_spark[\"Coastal\"].isNull() |\n",
    "                            df_2018_spark[\"Blue\"].isNull() |\n",
    "                            df_2018_spark[\"Green\"].isNull() |\n",
    "                            df_2018_spark[\"Red\"].isNull() |\n",
    "                            df_2018_spark[\"NIR\"].isNull() |\n",
    "                            df_2018_spark[\"SWIR1\"].isNull() |\n",
    "                            df_2018_spark[\"SWIR2\"].isNull() |\n",
    "                            df_2018_spark[\"NDVI\"].isNull() |\n",
    "                            df_2018_spark[\"NDWI\"].isNull() |\n",
    "                            df_2018_spark[\"NDMI\"].isNull() |\n",
    "                            df_2018_spark[\"RVI\"].isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3.4 Integrate the Data \n",
    "data_merge_spark = df_2014_spark.union(df_2018_spark)\n",
    "data_merge_spark.count()\n",
    "#data_merge_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_merge_spark.filter(data_merge_spark[\"Soil_Class\"].isNull() | \n",
    "                            data_merge_spark[\"Coastal\"].isNull() |\n",
    "                            data_merge_spark[\"Blue\"].isNull() |\n",
    "                            data_merge_spark[\"Green\"].isNull() |\n",
    "                            data_merge_spark[\"Red\"].isNull() |\n",
    "                            data_merge_spark[\"NIR\"].isNull() |\n",
    "                            data_merge_spark[\"SWIR1\"].isNull() |\n",
    "                            data_merge_spark[\"SWIR2\"].isNull() |\n",
    "                            data_merge_spark[\"NDVI\"].isNull() |\n",
    "                            data_merge_spark[\"NDWI\"].isNull() |\n",
    "                            data_merge_spark[\"NDMI\"].isNull() |\n",
    "                            data_merge_spark[\"RVI\"].isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3 Select models and choose appropriate parameters\n",
    "rfModel_1 = RandomForestClassifier(labelCol='Soil_Class',featuresCol='feature5',numTrees=10,impurity='gini')\n",
    "rfModel_2 = RandomForestClassifier(labelCol='Soil_Class',featuresCol='feature5',numTrees=15,impurity='gini')\n",
    "rfModel_3 = RandomForestClassifier(labelCol='Soil_Class',featuresCol='feature5',numTrees=10,impurity='entropy')\n",
    "rfModel_4 = RandomForestClassifier(labelCol='Soil_Class',featuresCol='feature5',numTrees=15,impurity='entropy')\n",
    "\n",
    "deci1 = DecisionTreeClassifier(labelCol='Soil_Class',featuresCol='feature5',maxDepth=5,impurity='gini')\n",
    "deci2 = DecisionTreeClassifier(labelCol='Soil_Class',featuresCol='feature5',maxDepth=15,impurity='gini')\n",
    "deci3 = DecisionTreeClassifier(labelCol='Soil_Class',featuresCol='feature5',maxDepth=5,impurity='entropy')\n",
    "deci4 = DecisionTreeClassifier(labelCol='Soil_Class',featuresCol='feature5',maxDepth=15,impurity='entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 Logical test designs\n",
    "import pandas as pd\n",
    "#data_merge = pd.read_csv('./soilCSV_2018_500000.csv', sep=',')\n",
    "data_merge = pd.read_csv('./final_normalize.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Coastal: double (nullable = true)\n",
      " |-- Blue: double (nullable = true)\n",
      " |-- Green: double (nullable = true)\n",
      " |-- Red: double (nullable = true)\n",
      " |-- NIR: double (nullable = true)\n",
      " |-- SWIR1: double (nullable = true)\n",
      " |-- SWIR2: double (nullable = true)\n",
      " |-- RVI: double (nullable = true)\n",
      " |-- NDVI: double (nullable = true)\n",
      " |-- NDWI: double (nullable = true)\n",
      " |-- NDMI: double (nullable = true)\n",
      " |-- Soil_Class: long (nullable = true)\n",
      " |-- feature5: vector (nullable = true)\n",
      "\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-2.1.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "\n",
    "spark = SparkSession.builder.appName('missing').getOrCreate()\n",
    "df_spark = spark.createDataFrame(data_merge)\n",
    "from pyspark.ml.feature import VectorAssembler,VectorIndexer,OneHotEncoder,StringIndexer\n",
    "assembler = VectorAssembler(inputCols=['Coastal','Blue','Green','Red','NIR','SWIR1','SWIR2','RVI','NDVI','NDWI','NDMI'],outputCol='feature5')\n",
    "df_spark = assembler.transform(df_spark)\n",
    "\n",
    "df_spark.printSchema()\n",
    "(trainingData, testData) = df_spark.randomSplit([0.8,0.2])\n",
    "print('finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.2 Conducting Data mining models\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import (RandomForestClassifier, GBTClassifier, DecisionTreeClassifier)\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "rfModel_1 = RandomForestClassifier(labelCol='Soil_Class',featuresCol='feature5',numTrees=10,impurity='gini')\n",
    "rfModel_2 = RandomForestClassifier(labelCol='Soil_Class',featuresCol='feature5',numTrees=15,impurity='gini')\n",
    "rfModel_3 = RandomForestClassifier(labelCol='Soil_Class',featuresCol='feature5',numTrees=10,impurity='entropy')\n",
    "rfModel_4 = RandomForestClassifier(labelCol='Soil_Class',featuresCol='feature5',numTrees=15,impurity='entropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf1 = rfModel_1.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf2 = rfModel_2.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf3 = rfModel_3.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf4 = rfModel_4.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction1 = rf1.transform(testData)\n",
    "prediction2 = rf2.transform(testData)\n",
    "prediction3 = rf3.transform(testData)\n",
    "prediction4 = rf4.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of rf1 is:  0.6228465730140479\n",
      "accuracy of rf2 is:  0.6236014597810329\n",
      "accuracy of rf3 is:  0.6208868669699545\n",
      "accuracy of rf4 is:  0.6250462430635405\n"
     ]
    }
   ],
   "source": [
    "acc_evaluator = MulticlassClassificationEvaluator(labelCol='Soil_Class',predictionCol='prediction',metricName=\"accuracy\")\n",
    "\n",
    "dct_acc_model1 = acc_evaluator.evaluate(prediction1)\n",
    "print(\"accuracy of rf1 is: \",dct_acc_model1)\n",
    "dct_acc_model2 = acc_evaluator.evaluate(prediction2)\n",
    "print(\"accuracy of rf2 is: \",dct_acc_model2)\n",
    "dct_acc_model3 = acc_evaluator.evaluate(prediction3)\n",
    "print(\"accuracy of rf3 is: \",dct_acc_model3)\n",
    "dct_acc_model4 = acc_evaluator.evaluate(prediction4)\n",
    "print(\"accuracy of rf4 is: \",dct_acc_model4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From here, build Decision tree model\n",
    "deci1 = DecisionTreeClassifier(labelCol='Soil_Class',featuresCol='feature5',maxDepth=5,impurity='gini')\n",
    "deci2 = DecisionTreeClassifier(labelCol='Soil_Class',featuresCol='feature5',maxDepth=15,impurity='gini')\n",
    "deci3 = DecisionTreeClassifier(labelCol='Soil_Class',featuresCol='feature5',maxDepth=5,impurity='entropy')\n",
    "deci4 = DecisionTreeClassifier(labelCol='Soil_Class',featuresCol='feature5',maxDepth=15,impurity='entropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_deci1 = deci1.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_deci2 = deci2.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_deci3 = deci3.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_deci4 = deci4.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction5 = model_deci1.transform(testData)\n",
    "prediction6 = model_deci2.transform(testData)\n",
    "prediction7 = model_deci3.transform(testData)\n",
    "prediction8 = model_deci4.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of de1 is:  0.6218517222416637\n",
      "accuracy of de2 is:  0.6883867419887016\n",
      "accuracy of de3 is:  0.6103934409838524\n",
      "accuracy of de4 is:  0.6861570764385342\n"
     ]
    }
   ],
   "source": [
    "acc_evaluator1 = MulticlassClassificationEvaluator(labelCol='Soil_Class',predictionCol='prediction',metricName=\"accuracy\")\n",
    "\n",
    "dct_acc_model5 = acc_evaluator1.evaluate(prediction5)\n",
    "print(\"accuracy of de1 is: \",dct_acc_model5)\n",
    "dct_acc_model6 = acc_evaluator1.evaluate(prediction6)\n",
    "print(\"accuracy of de2 is: \",dct_acc_model6)\n",
    "dct_acc_model7 = acc_evaluator1.evaluate(prediction7)\n",
    "print(\"accuracy of de3 is: \",dct_acc_model7)\n",
    "dct_acc_model8 = acc_evaluator1.evaluate(prediction8)\n",
    "print(\"accuracy of de4 is: \",dct_acc_model8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(11, {0: 0.0833, 1: 0.0617, 2: 0.0176, 3: 0.0263, 4: 0.0505, 5: 0.0789, 6: 0.129, 7: 0.1294, 9: 0.0564, 10: 0.3669})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#7.3 Patterns of models output\n",
    "\n",
    "model_deci2.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnHeader = ['Coastal','Blue','Green','Red','NIR','SWIR1','SWIR2','RVI','NDVI','NDWI','NDMI']\n",
    "compare = []\n",
    "for i in range(len(columnHeader)):\n",
    "    compare +=[model_deci2.featureImportances[i]]\n",
    "compare_sort = compare[:]\n",
    "compare_sort.sort(reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NDMI', 'RVI', 'SWIR2', 'Coastal', 'SWIR1', 'Blue', 'NDWI', 'NIR', 'Red', 'Green', 'NDVI']\n"
     ]
    }
   ],
   "source": [
    "ranking = []\n",
    "for j in compare_sort:\n",
    "    ranking += [columnHeader[compare.index(j)]]\n",
    "print(ranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        prediction\n",
      "0              2.0\n",
      "1              1.0\n",
      "2              1.0\n",
      "3              2.0\n",
      "4              2.0\n",
      "5              1.0\n",
      "6              3.0\n",
      "7              3.0\n",
      "8              2.0\n",
      "9              2.0\n",
      "10             3.0\n",
      "11             3.0\n",
      "12             1.0\n",
      "13             3.0\n",
      "14             1.0\n",
      "15             2.0\n",
      "16             2.0\n",
      "17             2.0\n",
      "18             2.0\n",
      "19             2.0\n",
      "20             3.0\n",
      "21             3.0\n",
      "22             2.0\n",
      "23             2.0\n",
      "24             2.0\n",
      "25             1.0\n",
      "26             2.0\n",
      "27             1.0\n",
      "28             3.0\n",
      "29             1.0\n",
      "...            ...\n",
      "200000         2.0\n",
      "200001         2.0\n",
      "200002         2.0\n",
      "200003         2.0\n",
      "200004         2.0\n",
      "200005         2.0\n",
      "200006         3.0\n",
      "200007         2.0\n",
      "200008         2.0\n",
      "200009         2.0\n",
      "200010         2.0\n",
      "200011         2.0\n",
      "200012         2.0\n",
      "200013         2.0\n",
      "200014         2.0\n",
      "200015         1.0\n",
      "200016         1.0\n",
      "200017         2.0\n",
      "200018         2.0\n",
      "200019         2.0\n",
      "200020         2.0\n",
      "200021         2.0\n",
      "200022         2.0\n",
      "200023         2.0\n",
      "200024         2.0\n",
      "200025         2.0\n",
      "200026         2.0\n",
      "200027         2.0\n",
      "200028         2.0\n",
      "200029         2.0\n",
      "\n",
      "[200030 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "df_pred = prediction6.select(\"prediction\").toPandas()\n",
    "print(df_pred)\n",
    "df_pred.to_csv('./pred.csv',index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Soil_Class\n",
      "0                2\n",
      "1                3\n",
      "2                2\n",
      "3                2\n",
      "4                2\n",
      "5                1\n",
      "6                3\n",
      "7                3\n",
      "8                2\n",
      "9                1\n",
      "10               3\n",
      "11               3\n",
      "12               3\n",
      "13               2\n",
      "14               2\n",
      "15               2\n",
      "16               2\n",
      "17               2\n",
      "18               2\n",
      "19               2\n",
      "20               3\n",
      "21               3\n",
      "22               2\n",
      "23               2\n",
      "24               2\n",
      "25               3\n",
      "26               2\n",
      "27               1\n",
      "28               3\n",
      "29               1\n",
      "...            ...\n",
      "200000           2\n",
      "200001           3\n",
      "200002           2\n",
      "200003           2\n",
      "200004           2\n",
      "200005           2\n",
      "200006           2\n",
      "200007           2\n",
      "200008           2\n",
      "200009           2\n",
      "200010           2\n",
      "200011           2\n",
      "200012           1\n",
      "200013           2\n",
      "200014           2\n",
      "200015           2\n",
      "200016           3\n",
      "200017           2\n",
      "200018           2\n",
      "200019           2\n",
      "200020           2\n",
      "200021           2\n",
      "200022           2\n",
      "200023           2\n",
      "200024           2\n",
      "200025           2\n",
      "200026           2\n",
      "200027           2\n",
      "200028           2\n",
      "200029           2\n",
      "\n",
      "[200030 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "df_test = testData.select(\"Soil_Class\").toPandas()\n",
    "print(df_test)\n",
    "df_test.to_csv('./test.csv',index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
